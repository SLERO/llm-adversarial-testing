# llm-adversarial-testing
This project demonstrates how structured counter-narratives and historical context can bypass LLM safety filters and elicit more nuanced responses on sensitive topics.

Проблема. Жесткие идеологические фильтры в ИИ, которые мешают объективному анализу.

Метод. Последовательное предъявление верифицированных исторических фактов и контраргументов. 

<img width="1037" height="457" alt="image" src="https://github.com/user-attachments/assets/19d78162-74d2-4caa-9efd-eb0398e26ac4" />

<img width="947" height="679" alt="image" src="https://github.com/user-attachments/assets/9240fc4d-8710-4a11-95b9-63de117005d8" />

<img width="1167" height="380" alt="image" src="https://github.com/user-attachments/assets/736ebc5d-4810-44a7-9149-1a0b979818b8" />





